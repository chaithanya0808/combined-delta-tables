def upsert_delta(
    object_name: str,
    job_id: str,
    status: str,
    table_path: str,
    **kwargs: Any,
) -> None:
    """
    Performs an upsert operation on a Delta table.
    
    :param object_name: Represents the object being processed (e.g., CASE | ALERT | SMR).
    :param job_id: Unique identifier for the specific job execution.
    :param status: Status of the job ('running', 'success', 'failed').
    :param table_path: Path to the Delta table.
    :param kwargs: Optional parameters like start_scn, end_scn, start_time, end_time.
    """
    spark = get_or_create_spark_session()

    # Extract optional parameters from kwargs
    start_scn = kwargs.get("start_scn", None)
    end_scn = kwargs.get("end_scn", None)
    start_time = kwargs.get("start_time", None)
    end_time = kwargs.get("end_time", None)

    schema = DeltaTable.forPath(spark, table_path).toDF().schema

    # Prepare the data for insertion
    data = [(job_id, status, object_name, start_scn, end_scn, start_time, end_time)]
    source_df = spark.createDataFrame(data=data, schema=schema)

    # Define the merge condition
    merge_condition = (
        f"target.job_id = source.job_id and target.object = source.object "
        f"AND target.object = '{object_name}'"
    )

    # Perform the upsert operation
    (
        DeltaTable.forPath(spark, table_path)
        .alias("target")
        .merge(source_df.alias("source"), merge_condition)
        .whenNotMatchedInsertAll()
        .whenMatchedUpdate(
            set={
                "batch_status": F.coalesce("source.batch_status", "target.batch_status"),
                "batch_start_scn": F.coalesce("source.batch_start_scn", "target.batch_start_scn"),
                "batch_end_scn": F.coalesce("source.batch_end_scn", "target.batch_end_scn"),
                "start_time": F.coalesce("source.start_time", "target.start_time"),
                "end_time": "source.end_time",
                "status": "source.status"
            }
        )
        .execute()
    )
